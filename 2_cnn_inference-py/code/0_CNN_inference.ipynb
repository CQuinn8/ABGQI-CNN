{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose:\n",
    "- Use a directory or directories of mel spectrograms to generate ABGQI predictions from ABGQI-CNN. \n",
    "- Each mel spec directory will result in one csv. \n",
    "- Each row of the csv corresponds to a single mel spec.\n",
    "\n",
    "### Notes\n",
    "- uses a directory of the 6 demo mels (combined in a single dir)\n",
    "- adjust CPUs to desired amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "'''\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "'''\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import glob\n",
    "import joblib\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. USER INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../cnn_training/results/ABGQI-CNN\n",
      "../results/\n"
     ]
    }
   ],
   "source": [
    "#output directory\n",
    "out_dir = '../results/'\n",
    "\n",
    "# trained models\n",
    "wd ='../../cnn_training/results/'\n",
    "checkpoint_path = os.path.join(wd, 'ABGQI-CNN') # IMGNET + S2L CVFOLDS\n",
    "\n",
    "# data dir for images to predict class\n",
    "data_path = '../../melspec_generation/results/all_melspecs/'\n",
    "\n",
    "# number of cores\n",
    "cpus = 2\n",
    "\n",
    "print(checkpoint_path)\n",
    "print(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images    \n",
    "img_width, img_height, img_depth = 224, 224, 3 # 224 pixels x 224 pixels x 3 bands (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of images to predict at once \n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read in a file path\n",
    "def process_path(file_path, IMG_HEIGHT, IMG_WIDTH):\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img, IMG_HEIGHT, IMG_WIDTH)\n",
    "    return img #, label\n",
    "\n",
    "# function that interpretes image after being provided a path in process_path\n",
    "def decode_img(img, IMG_HEIGHT, IMG_WIDTH):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_jpeg(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "  return tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load trained model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LOAD MODEL\n",
    "model = tf.keras.models.load_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.40_224 (Functi (None, 7, 7, 1792)        4363712   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 8965      \n",
      "=================================================================\n",
      "Total params: 4,372,677\n",
      "Trainable params: 4,324,741\n",
      "Non-trainable params: 47,936\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Set up sigmoid and softmax prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "png_dirs = []\n",
    "for filename in os.listdir(data_path):\n",
    "    if os.path.isdir(os.path.join(data_path, filename)):\n",
    "        png_dirs.append(filename)\n",
    "if len(png_dirs) == 0: \n",
    "    png_dirs.append(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../melspec_generation/results/all_melspecs/']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "png_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../results/demo_inference'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../cnn_training/results/ABGQI-CNN\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../results/\n",
      "Number of pngs 6\n",
      "directory already exists. Moving to predictions...\n",
      "../results/sigmoid\\all_melspecs\n",
      "Saved this pkl at: ../results/all_melspecs \n",
      "Length was: 6\n"
     ]
    }
   ],
   "source": [
    "# iterate through each directory (class or wav dir with mfccs within)\n",
    "for i,f in enumerate(png_dirs):\n",
    "    temp_dir = f\n",
    "    pngs = glob.glob(temp_dir + \"*.png\")   \n",
    "    temp_class = temp_dir.split('/')[-2] # png parent dir\n",
    "    print()\n",
    "    print(out_dir)\n",
    "    print(\"Number of pngs\", len(pngs)) \n",
    "    \n",
    "    # check if the parent output dir exists\n",
    "    if(os.path.exists(out_dir)):\n",
    "        print(\"directory already exists. Moving to predictions...\")            \n",
    "    else:\n",
    "        os.mkdir(out_dir)\n",
    "        print(\"Created :\", out_dir)\n",
    "        \n",
    "    # list to hold each dir of predictions (either all pngs in class or mfccs in wav)\n",
    "    sigmoid_pred_lst = []\n",
    "    \n",
    "    # file path for class pkl\n",
    "    out_path_temp = os.path.join(os.path.join(out_dir, \"sigmoid\",  temp_class))\n",
    "    print(out_path_temp)\n",
    "    \n",
    "    # iterate through each melspec in file directory\n",
    "    for j in range(len(pngs)):      \n",
    "        temp_png = pngs[j] # png        \n",
    "        \n",
    "        #check if pkl (prediction) for file exists\n",
    "        if os.path.isfile(out_path_temp):\n",
    "            pass # skip file \n",
    "        \n",
    "        # enter prediction \n",
    "        else:\n",
    "            # process image\n",
    "            img_ = process_path(temp_png, IMG_HEIGHT = img_height, IMG_WIDTH = img_width)\n",
    "            img_ = tf.reshape(img_, shape= (1, img_height, img_width, img_depth))\n",
    "            \n",
    "            # get predictions\n",
    "            pred = model.predict(img_, verbose=0, steps=1, callbacks=None, max_queue_size=10, \n",
    "                  workers=cpus, use_multiprocessing=False)\n",
    "\n",
    "            sigmoid_pred = tf.math.sigmoid(pred).numpy()\n",
    "            sigmoid_pred_lst.append(sigmoid_pred)\n",
    "      \n",
    "    # save label preds\n",
    "    flat_sigmoid = [item for sublist in sigmoid_pred_lst for item in sublist]\n",
    "    df_sigmoid = pd.DataFrame(flat_sigmoid)\n",
    "    df_sigmoid.columns = [\"Anthrophony\", \"Biophony\", \"Geophony\", \"Other\", \"Interference\"]\n",
    "    \n",
    "    # write directory csv \n",
    "    df_sigmoid.to_csv(os.path.join(out_dir, temp_class + '.csv'), index = False)     \n",
    "    print('Saved this pkl at:',os.path.join(out_dir, temp_class), \"\\nLength was:\",len(sigmoid_pred_lst))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ABG-cnn_tf230] *",
   "language": "python",
   "name": "conda-env-ABG-cnn_tf230-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
